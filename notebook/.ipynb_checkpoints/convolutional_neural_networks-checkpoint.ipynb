{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "Currently following this tutorial: https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "The layers of convolutional neural networks are, unlike in classical (deep) neural networks, 3-dimensional, having width, height and depth.\n",
    "<br>\n",
    "Width and heigth correspond to the width and height of the images processed and the depth can correspond to the brightness, greyscale or the three primary colors (RGB).\n",
    "<br>\n",
    "<br>\n",
    "Another major difference is, that the neurons in one layer needn't connect to all neurons in the next layer. While there often are fully connected layers in the end, for the convolutional layers usually several neurons in one layer are connected to only few or one in the next layer.\n",
    "<br>\n",
    "The convolutional layers are in the beginning and their job is the recognition of features. The first layers might only recognize simple things like lines or edges, but the further you go the more complex the shapes become (a head, a leg, a leaf?). The job of the fully connected layers is to put all this information together and use it to make a good prediction on whatever the goal of the network is (like detecting pedestrians)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The convolution\n",
    "In mathematics a convolution is the combination of two functions in a certain way to result in a third function.\n",
    "<br>\n",
    "In machine learning a convolution is the combination of the input image (or the output of a previous layer) and a filter/kernel. A filter has a certain job, like detecting edges. Combined with the input image the filter detects edges in the image and passes the result on to the next layer for further processing. The result is often called a feature map, since it contains the features detected by the previous layer(s).\n",
    "<br>\n",
    "<br>\n",
    "Further (and quite extensive) information can be found here: http://timdettmers.com/2015/03/26/convolution-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filters / Kernels\n",
    "A filter is used to extract features from an image or from results returned by previous layers. The terms filter and kernel can be used interchangably.\n",
    "<br>\n",
    "The filter almost never spans the entire image, usually it is quite small e.g. 3x3 pixels. The thought behind this is to limit the focus of processing to only small parts of the image at a time and then put this information together, which is why the area of the filter is sometimes called the receptive field.\n",
    "<br>\n",
    "To still cover the entire field the filter is moved across the image. You could imagine this as blocking out everything in the image except the filter area. When you are done looking at this part you move the filter, so the area you look at changes. With every step the filter is moved across the image until every part of it has been passed through the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride\n",
    "The stride is the amount of pixels a filter moves over the layer with each step. If you have a stride of 1, the filter moves one pixel at a time. If you have a stride of 2 the filter moves 2 pixels with each step and so on.\n",
    "<br>\n",
    "This is repeated until every part of the image has been filtered.\n",
    "\n",
    "### Padding\n",
    "Depending on the size of the filter (if it is bigger than 1x1), you will reduce the size of your input by filtering. Imagine having a 4x4 input and sliding a 3x3 filter over it. Since the filter is bigger than 1x1 it can not have every single pixel as it's center. This means instead of applying the filter with all 16 pixels, in this case we could only apply it with 4 pixels at it's center. In other words the output would not be 4x4 anymore but 2x2.\n",
    "<br>\n",
    "This can, especially over several convolution layers, reduce the size of the resulting feature maps significantly. To prevent this you can pad your image, meaning you add \"dummy pixels\" around it before filtering.\n",
    "<br>\n",
    "Often padding is used to keep the original input size the same, despite filter and stride size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling layer\n",
    "After a convolution layer you usually have a pooling layer. This layer reduces the resulting feature map, it pools the resulting values to get a smaller result.\n",
    "<br>\n",
    "For example you can have a pooling layer with a 2x2 receptive field and a stride of 2. If you look at a 4x4 image you have to slide the pool across the feature map 4 times, each time looking at 4 totally different pixels.\n",
    "Since the goal of this layer is to reduce the feature map's size you usually reduce all the values looked at up the pool (in this case 4) into a single value.\n",
    "<br>\n",
    "There are several ways to do this, you can simply select the max value (max pool) or calculate the arithmetic mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Conv2D, Activation, MaxPool2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 27, 27, 6)         30        \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 27, 27, 6)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 6)         0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 1014)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                10150     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 10,180\n",
      "Trainable params: 10,180\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img_shape = (28, 28, 1)\n",
    "\n",
    "model = Sequential()\n",
    "# Uses 6 filters that are 2x2 in size\n",
    "model.add(Conv2D(6, 2, input_shape=img_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPool2D(2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 6s 1us/step\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 5.8909 - acc: 0.6258 - val_loss: 5.3045 - val_acc: 0.6655\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 5.2102 - acc: 0.6717 - val_loss: 4.0214 - val_acc: 0.7405\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 3.8150 - acc: 0.7553 - val_loss: 3.6613 - val_acc: 0.7665\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 7s 121us/step - loss: 3.6475 - acc: 0.7669 - val_loss: 3.6006 - val_acc: 0.7697\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 3.5847 - acc: 0.7710 - val_loss: 3.5824 - val_acc: 0.7690\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 7s 120us/step - loss: 2.2794 - acc: 0.8490 - val_loss: 1.9058 - val_acc: 0.8724\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 1.8296 - acc: 0.8760 - val_loss: 1.8363 - val_acc: 0.8739\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 1.7503 - acc: 0.8787 - val_loss: 1.8027 - val_acc: 0.8715\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.5543 - acc: 0.9424 - val_loss: 0.1186 - val_acc: 0.9678\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 7s 119us/step - loss: 0.0964 - acc: 0.9721 - val_loss: 0.1123 - val_acc: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x320b828>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"acc\"])\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "- How do filters work (detect lines etc.)\n",
    "<br>\n",
    "<br>\n",
    "- Provide a notebook that helps other beginners more than most blogposts currently do\n",
    "- Hard to explain this without adding images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
